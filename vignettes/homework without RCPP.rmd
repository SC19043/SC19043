---
title: "homework"
author: '19043'
date: "2020.1.1"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r  echo=FALSE}
library("knitr")
library("bootstrap")
```

## Qeustion

Use knitr to produce texts,figures and tables

## Answer

First, to produce texts, We take it as a example  to get the mean of the score of vec named $\bar{X}$ in bootstrap package:

```{r, echo=TRUE}
scor
```

So $\bar{X}$ epuals `r mean(scor[,2])`

Then, we produce two figures in the following  
1. we get the image of the function $y=x^3$ on the interval $[-5,5]$

```{r echo=TRUE}
curve(x^3,-5,5,xlab="x",ylab="y")
abline(v=0,h=0,lty=2)
```

2. we get the figure of presidents given in R package

```{r  echo=TRUE}
plot(presidents)
```

Finally, we output the fist 15 lines of the table of scor in bootstrap package

```{r  echo=TRUE}
kable(scor[(1:15),])
```

Thus, by using these functions, we can produce texts, figures,and tables.


## Qeustion

<font size=5 color=blue>3.4</font>$\quad$The Rayleigh density is $f(x)=\frac{x}{\sigma^2}e^{-x^2/2\sigma^2},\quad x\ge0,\sigma>0.$
Develop an algorithm to generate random samples from a Rayleigh($\sigma$) distribution. Generate Rayleigh($\sigma$) samples for several choices of $\sigma$>0 and check that the mode of the generated samples is close to the theoretical mode $\sigma$ (check the histogram).

<font size=5 color=blue>3.11</font>$\quad$Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have $N(0,1)$ and $N(3,1)$ distributions with mixing probabilities $p_1$ and $p_2=1-p_1$. Graph the histogram of the sample with density superimposed, for $p_1=0.75$. Repeat with different values for $p_1$ and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of $p_1$ that produce bimodal mixtures. 

<font size=5 color=blue>3.18</font>$\quad$Write a function to generate a random sample from a $W_d(\sum,n)$ (Wishart) distribution for $n>d+1\ge1$, based on Bartlett's decomposition.

## Answer to 3.4

Here, we use inverse transformation method.
We define Rayleigh(n,sigma) as the function to generate n random samples from a Rayleigh($\sigma$) distribution.

It is easy to see that the cdf of a Rayleigh(s) distribution is 
$F_X(x)=1-e^{-x^2/2\sigma^2},\quad x\ge0,\sigma>0$.
So,the inverse transformation is $F_X^{-1}(u)=\sqrt{-2\sigma^2log(1-u)}$.

```{r}
Rayleigh <- function(n, sigma){
  u<-runif(n)
  x<-sqrt(-2*sigma^2*log(1-u))
  hist(x,probability = TRUE)
  y=seq(0,4000,.0005)
  lines(y,y*exp(-y^2/(2*sigma^2))/sigma^2)
}
```

First,we generate 10000 random samples with $\sigma=1$

```{r}
Rayleigh(10000,1)
```

Second,we generate 100000 random samples with $\sigma=0.01$

```{r}
Rayleigh(100000,0.1)
```

Third,we generate 1000 random samples with $\sigma=10$

```{r}
Rayleigh(1000,10)
```

Finally,we generate 1000 random samples with $\sigma=1000$

```{r}
Rayleigh(1000,1000)
```

We generated different numbers of samples for different $\sigma$, becaues the line may be interrupted if the number of sample is too small for some $\sigma$.

Anyway,we can find that the samples from the function "Rayleigh" is close to the real situation.

## Answer to 3.11

To solve this priblem, we are going to use transformation methods.
We define mix(n,p) as the function to generate n random samples from this normal location mixture.

```{r}
mix<-function(n,p){
  x1<-rnorm(n,0,1)
  x2<-rnorm(n,3,1)
  r<-rbinom(n,1,prob=p)
  z<-r*x1+(1-r)*x2
  hist(z,probability = TRUE,main = p)
  z<-seq(-4,7,.05)
  lines(z,p*dnorm(z,0,1)+(1-p)*dnorm(z,3,1))
}
```

First, we let $p_1=0,75$

```{r}
mix(1000,0.75)
```

Then,we let $p_1$ equals $0,0.05,0.1,0.15,0.2,...0.95,1$

```{r}
p0<-seq(0,1,0.05)
for (i in p0){
  mix(1000,i)
}

```

Thus,when $0.2<p_1<0.8$, the empirical distribution of the mixture appears to be bimodal.

## Answer to 3.18
We define wdsample(Sigma,n,d) as the function to generate a random sample from a $W_d(\sum,n)$ (Wishart) distribution for $n>d+1\ge1$, based on Bartlett's decomposition.

```{r}
library(MASS)
wdsample<-function(Sigma,n,d){
  T <- matrix(0, nrow = d, ncol = d)    #define a d*d matrix
  for(i in (1:d)){
    T[i,i]=sqrt(rchisq(1,n-i+1))
    for(j in (1:i-1)){
      T[i,j]=rnorm(1,0,1)
    }
  }
  L=chol(Sigma)
  A=T%*%t(T)
  L%*%A%*%t(L)
}
```
Then, we let n=6,d=3,$\sum=\begin{pmatrix}3 & 1 & 0\\1 & 4 & 0\\0 & 0 & 5\end{pmatrix}$. We use this matrix beacause it is symmetric, positive and  not  special.

```{r}
Sigma <- matrix(nr=3,nc=3) 
Sigma[1,]<-c(3,1,0)
Sigma[2,]<-c(1,4,0)
Sigma[3,]<-c(0,0,5)
wdsample(Sigma,6,3)
```

## Qeustion 5.1

Comptue a Monte Carlo estimate of
$$
\theta=\int_{0}^{\pi/3}\sin t\,dt
$$
and compare your estimate with the exact value of the integral.

## Answer

We first compute the exact value of the intergral:
```{r}
theta<-integrate(sin,0,pi/3)$value
theta
```

Then, we compute its Monte Carlo estimate:
```{r}
set.seed(0)
m<-10000;x<-runif(m,min=0,max=pi/3)
theta.hat<-mean(sin(x)*pi/3)
print(theta.hat)
```
So, the exact value $\theta$  is  `r theta` ,while the estimate $\hat{\theta}$ is `r theta.hat`.



## Question 5.10

Use Monte Carlo integration with antithetic variables to estimate 
$$
\int_0^1\frac{e^{-x}}{1+x^2}\,dx,
$$
and find the approximate reduction in variance as a percentage of the variance without variance reduction.

## Answer
```{r}
set.seed(1)
MC.Phi<-function(R,antithetic = TRUE){
  u <- runif(R/2,0,1)
  if (!antithetic) v <- runif(R/2,0,1) else v <- 1 - u
  u <- c(u, v)
  g <-exp(-u)/(u^2+1)
  mean(g)
}
m <- 1000
MC1 <- MC2 <- numeric(m)
for (i in 1:m) {
  MC1[i] <- MC.Phi(m, antithetic = FALSE)
  MC2[i] <- MC.Phi(m)
}
var(MC1)
var(MC2)
(var(MC1)-var(MC2))/var(MC1)
```
So, the antithetic variable approached approximately 96.8% reduction in variance.



## Question 5.15
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

In Example 5.10 our best result was obtained with importance function $f_3(x)=e^{-x}/(1-e^{-1}),\,0<x<1$.  Now devide the interval $(0,1)$ into five subintervals,$(j/5,(j+1)/5),j=0,1,2,3,4$. 

Then on the $j^{th}$ subinterval variables are generated from the density
$$
\frac{5e^{-x}}{1-e^{-1}},\qquad \frac{j-1}{5}<x<\frac{j}5.
$$
Then, we compute the estimate and estimated standard error.

## Answer

In this question, we need to change the cdf given into 
$$
\frac{e^{-x}}{e^{-(j-1)/5}-e^{-j/5}},\qquad \frac{j-1}{5}<x<\frac{j}5.
$$

```{r }
set.seed(4)
M<-10000;k<-5
r<-M/k  #replicates per stratum
N<-50  #number of times to repeat the estimation
T2<-numeric(k)
est<-matrix(0,N,2)
g<-function(x){exp(-x)/(1+x^2)*(x>0)*(x<1)}
for (i in 1:N){
  u<- runif(M)  #inverse transform method
  x<- -log(1-u*(1-exp(-1)))
  fg<-g(x)*(1-exp(-1))/exp(-x)
  est[i,1]<-mean(fg)
  for (t in 1:k){
    v<- runif(r)  #inverse transform method
    y<- -log(exp(-(t-1)/5)-v*(exp(-(t-1)/5)-exp(-t/5)))
    fg0<- g(y)*(exp(-(t-1)/5)-exp(-t/5))/exp(-y)
    T2[t]<-mean(5*fg0)
  }
  est[i,2]<-mean(T2)
}
apply(est,2,mean)
apply(est,2,sd)
```
So, the estimates are close to each other, but the standard error using stratified importance sampling became about 1/5 of the result in Example 10.

## Question

6.5  Suppose a $95\%$ symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the pribability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experimment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size=20. Compare your t-interval results with the simulation results in Example 6.4.(The t-interval should be more rebust to departures from normality than the interval for variance)

## Answer

If $X_1,X_2...,X_n$ is a random sample from a Normal$(\mu,\sigma^2)$ distribution, $n\ge2$, and $\bar X$ and $S^2$ is the sample mean and variance, then
$$
T=\frac{\bar X-\mu}{S/\sqrt n}\sim t(n-1)
$$
So, a $100(1-\alpha)\%$ confidence interval is given by $(\bar X-t_{\alpha/2}(n-1)\frac S{\sqrt n},\bar X+t_{\alpha/2}(n-1)\frac S{\sqrt n})$. In this question, to use a symmrtric t-interval to estimate the mean, we need to suppose that $T=\frac{\bar X-\mu}{S/\sqrt n}\sim t(n-1)$. In this way, we can deal with it like a random sample from a Normal distribution.

To estimate the coverage probility, we should use the conclusion that the mean of X from a  $\chi^2(2)$ distribution is 2.

```{r}
set.seed(123)
m <- 1000
n <- 20
alpha <- 0.05
UCL <- LCL <- numeric(m)
for (i in 1:m){
  x <- rchisq(n, df=2)
  x_bar <- mean(x)
  S <- sqrt(var(x))
  UCL[i] <- x_bar-S*qt(alpha/2,df=n-1)/sqrt(n)
  LCL[i] <- x_bar+S*qt(alpha/2,df=n-1)/sqrt(n)
} 
sum(LCL<2 & 2<UCL)    #the real mean is 2
mean(LCL<2 & 2<UCL)   #compare the real value with t-interval
```

In Example 6.4, to copmute the confidence interval for variance from a $\chi^2(2)$ distribution, we get it and estimate its confidence level with the code:

```{r}
set.seed(123)
n<-20
alpha<-0.05
UCL1<-replicate(1000,expr = {
  x<-rchisq(n,df=2)
  (n-1)*var(x)/qchisq(alpha,df=n-1)
  })
sum(UCL1>4)
mean(UCL1>4)
```

From the result, we can see the coverage probility is 0.908 smaller than 0.95. This is exactly right because the sample is from $\chi^2$, not Normal.

Compared with the result of Example 6.4 which is only 0.781, it is more rebust.

## Question

6.6  Estimate the 0.025, 0.05, 0.95 and 0.975 quantiles of the skewness $\sqrt{b_1}$ under normality by a Monte Carlo experiment. Compute the stantard error of the estimates from (2.14) using the normal approximation for the density (with exact variance formula). Compare the estimates quantiles with the quantiles if the large sample approximation $\sqrt{b_1}\approx N(0.6/n)$

## Answer

In this question, we generate samples from $N(0,1)$, and compute their skewness. Then, we compute the quantiles of the skewness gotten before and compare with the quantiles of $N(0,6/n)$.

```{r}
set.seed(1)
n <- 1000   #sample size
m <- 1000
sk <- function(x){
  #compute the sample skewness coeff
  xbar <- mean(x)
  m3 <- mean((x-xbar)^3)
  m2 <- mean((x-xbar)^2)
  return(m3/m2^1.5)
}
skew <- numeric(m)
for (i in 1:m){
  x <- rnorm(n)
  skew[i] <- sk(x)
}
C <- c(0.025, 0.05, 0.95,0.975)
est<-quantile(skew,probs = C)    #estimates of skewness
qnorm<-qnorm(c(0.025, 0.05, 0.95,0.975),mean=0,sd=sqrt(6/n))   #quantile of N(0,6/n)
sd<-C*(1-C)/(n*dnorm(C))
#standard of estimates from (2.14) using the normal approximation for the density 
knitr::kable(rbind(est,qnorm,sd),col.names = c('0.025','0.05','0.95','0.975'))
```

From this table, we can see that under large sample, the quantile is close to the quantile of $N(0,6/n)$. 

Also, the standard error is quite small.


## Queston 6.7

Estimate the power of the skewnwss test of normality against symmetric $Beta(\alpha,\alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(\nu)$? 

## Answer

In this experiment, the significance level is $\alpha=0.1$, and the sample size is $n=30$.

The containminates Beta distribution is denoted by 
$$
(1-\epsilon)Beta(1,1)+\epsilon Beta(100,100),\qquad 0\leq\epsilon\leq1.
$$

The containminates t distribution is denoted by 
$$
(1-\epsilon)t(1)+\epsilon t(100),\qquad 0\leq\epsilon\leq1.
$$

```{r}
alpha<-.1
n<-30
m<-2500
epsilon<-c(seq(0,.15,.01),seq(.15,1,.05))
N<-length(epsilon)
pwr<-pwr2<-numeric(N)
cv<-qnorm(1-alpha/2,0,sqrt(6*(n-2)/((n+1)*(n+3))))
sk <- function(x){
  #compute the sample skewness coeff
  xbar <- mean(x)
  m3 <- mean((x-xbar)^3)
  m2 <- mean((x-xbar)^2)
  return(m3/m2^1.5)
}
set.seed(123)
for (j in 1:N) {
  e<-epsilon[j]
  sktests<-sktests2<-numeric(m)
  for (i in 1:m){
    a<-sample(c(1,100),replace = TRUE, size = n, prob = c(1-e,e))
    x<-rbeta(n,a,a)
    y<-rt(n,a)
    sktests[i]<-as.integer(abs(sk(x))>=cv)
    sktests2[i]<-as.integer(abs(sk(y))>=cv)
  }
  pwr[j]<-mean(sktests)
  pwr2[j]<-mean(sktests2)
}
plot(epsilon,pwr,xlab = bquote(epsilon),ylab="power of beta",type = "b",ylim=c(0,1))
abline(h=.1,lty=3)
se<-sqrt(pwr*(1-pwr)/m)
lines(epsilon,pwr+se,lty=3)
lines(epsilon,pwr-se,lty=3)
plot(epsilon,pwr2,xlab = bquote(epsilon),ylab="power of t",type = "b",ylim = c(0,1))
abline(h=.1,lty=3)
se2<-sqrt(pwr2*(1-pwr2)/m)
lines(epsilon,pwr2+se2,lty=3)
lines(epsilon,pwr2-se2,lty=3)
```

We can see that the power of the skewnwss test of normality against symmetric $Beta(\alpha,\alpha)$ distributions is similar to the normality. But for $t(\nu)$, the result is quite different.

## Question 6.A

Use Monte Carlo simulation to investigate whether the empirical Type I error rate of t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^2(1)$, (ii) Uniform(0,2) and (iii) Exponential(rate=1). In each case,test $H_0:\mu=\mu_0$ vs $H_1:\mu\neq\mu_0$, where $\mu_0$ is the mean of $\chi^2(1)$, Uniform(0,2) and Exponential(1), respectively.

## Answer

```{r}
set.seed(3)
n<-20
alpha<-0.05
m<-10000
mu0<-1
p.chi<-p.unif<-p.exp<-numeric(m)
for (j in 1:m) {
  x<-rchisq(n,1)
  y<-runif(n,0,2)
  z<-rexp(n,1)
  testx<-t.test(x,mu=mu0)
  testy<-t.test(y,mu=mu0)
  testz<-t.test(z,mu=mu0)
  p.chi[j]<-testx$p.value
  p.unif[j]<-testy$p.value
  p.exp[j]<-testz$p.value
}
p.hat.chi<-mean(p.chi<alpha)
se.chi<-sqrt(p.hat.chi*(1-p.hat.chi)/m)
p.hat.unif<-mean(p.unif<alpha)
se.unif<-sqrt(p.hat.unif*(1-p.hat.unif)/m)
p.hat.exp<-mean(p.exp<alpha)
se.exp<-sqrt(p.hat.exp*(1-p.hat.exp)/m)
print(c(p.hat.chi,se.chi))
print(c(p.hat.unif,se.unif))
print(c(p.hat.exp,se.exp))
```

From this result, for $U(0,2)$, Type I error probality is close to the nominal rate.However, for $\chi^2(1)$ and Exponential(rate=1), the probality is not close.

## Discussion

If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?
        
What is the corresponding hypothesis test problem?
        
What test should we use? Z-test,  two-sample t-test, paired-t test or McNemar test?
        
What information is needed to test your hypothesis?
        
## Answer

Let $P_1$ and $P_2$ be the powers for two methods. Under this setting, the test hypotheses are $H_0:P_1=P_2\quad vs \quad H_1:P_1\neq P_2$.

Here, I want to use paired-t test. We need the information below:

(1)$R_1,R_2$:the rejection areas of the two methods.

(2)$T_1,T_2$:the statistics from sample $X_1,X_2,...,X_n$ in a experiment.

If we do m experiments, then we can get that the estimates of $P_1??P_2$ are denotes by
$$
\hat P_1=\frac1m\sum_{i=1}^m1(T_1^i\in R_1),\quad\hat P_2 = \frac1m\sum_{i=1}^m1(T_2^i\in R_2)
$$



## Question 7.6

Efron and Tibshirani discuss the scor test score data on 88 students who took examinations in five subjects. The first two tests (mechanics, vectors) were closed book and the last three tests (algebra, analysis, statitics) were open book. Each row of the data frame is a set of scores $(x_{i1},...,x_{i5})$ for the $i^{th}$ student. Use a panel display to display the scatter plots for each pair of test scores. Compare the plot with the sample correlation matrix. Obtain bootstrap estimates of standard errors for each of the following estimates: 
$$
\hat \rho_{12}=\hat \rho (mec,vec),\hat \rho_{34}=\hat \rho (alg,ana),\hat \rho_{35}=\hat \rho (alg,sta),\hat \rho_{45}=\hat \rho (ana,sta).
$$

```{r}
set.seed(123)
library(bootstrap)
B<-200        #number of replicates
n<-nrow(scor)
R<-matrix(0,B,4)
rho<-matrix(0,4,2)
rownames(rho)<-c("rho12","rho34","rho35","rho45")
colnames(rho)<-c("est","est.sd")
##par(mfrow = c(2, 2))
plot(scor$mec,scor$vec,xlab = "mec",ylab = "vec",cex=0.6)
plot(scor$alg,scor$ana,xlab = "alg",ylab = "ana",cex=0.6)
plot(scor$alg,scor$sta,xlab = "alg",ylab = "sta",cex=0.6)
plot(scor$ana,scor$sta,xlab = "ana",ylab = "sta",cex=0.6)
print(cor(scor))
for (b in 1:B) {
  i<-sample(1:n, size = n, replace = TRUE)
  mec<-scor$mec[i]
  vec<-scor$vec[i]
  alg<-scor$alg[i]
  ana<-scor$ana[i]
  sta<-scor$sta[i]
  R[b,1]<-cor(mec,vec)
  R[b,2]<-cor(alg,ana)
  R[b,3]<-cor(alg,sta)
  R[b,4]<-cor(ana,sta)
}
for (i in 1:4){
  rho[i,]<-c(mean(R[,i]),sd(R[,i]))
}
print(rho)
```

From the results, we can see that the bootstrap estimate of $\rho_{12},\rho_{34},\rho_{35},\rho_{45}$ are close to the sample correlation. And the estimate of standard error of $\rho_{12},\rho_{34},\rho_{35},\rho_{45}$ can be seen in the table named est.sd.

## Question 7.B

Repeat Project 7.A for the sample skewness statistic. Compare the coverage rates for normal populations (skewness 0) and $\chi^2(5)$ distributions (positive skewness).

7.A:
Conduct a Monte Carlo study estimate the coverage probilities of the standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval. Sample from a normal population and check the empirical coverage rates for the sample mean. Find the proportion of times that the confidence intervals miss on the left, and the proportion of times that the confidence intervals miss on the right.


## Answer

The real skewness of $\chi^2(5)$ is $\frac{4}{\sqrt{10}}$.

```{r}
set.seed(0)
library(boot)
n<-100
m<-1000  
sk.chi5<-4/sqrt(10)
ci.normal<-ci.chisq<-matrix(0,m,6)
norm.miss<-chi.miss<-matrix(0,3,3)
rownames(norm.miss)<-rownames(chi.miss)<-c("normal CI","basic CI","percentile CI")
colnames(norm.miss)<-colnames(chi.miss)<-c("coverage rate","miss on the left","miss on the right")
sk <- function(x,i){
  #compute the sample skewness coeff
  xbar <- mean(x[i])
  m3 <- mean((x[i]-xbar)^3)
  m2 <- mean((x[i]-xbar)^2)
  return(m3/m2^1.5)
}
for (k in 1:m) {
  x<-rnorm(n)
  y<-rchisq(n,5)
  boot.obj.norm<-boot(x,R=2000,statistic = function(x,i){sk(x,i)})
  boot.obj.chi<-boot(y,R=2000,statistic = function(y,i){sk(y,i)})
  ci.norm<-boot.ci(boot.obj.norm,type = c("norm","basic","perc"))
  ci.chi<-boot.ci(boot.obj.chi,type = c("norm","basic","perc"))
  ci.normal[k,]<-c(ci.norm$normal[2],ci.norm$normal[3],ci.norm$basic[4],ci.norm$basic[5],ci.norm$percent[4],ci.norm$percent[5])
  ci.chisq[k,]<-c(ci.chi$normal[2],ci.chi$normal[3],ci.chi$basic[4],ci.chi$basic[5],ci.chi$percent[4],ci.chi$percent[5])
}
for (j in 1:3) {
  norm.miss[j,]<-c(mean(0>ci.normal[,2*j-1] & 0<ci.normal[,2*j]) ,mean(0<ci.normal[,2*j-1]),mean(0>ci.normal[,2*j]))
  chi.miss[j,]<-c(mean(sk.chi5>ci.chisq[,2*j-1] & sk.chi5<ci.chisq[,2*j]) ,mean(sk.chi5<ci.chisq[,2*j-1]),mean(sk.chi5>ci.chisq[,2*j]))
}
knitr::kable(norm.miss, caption = "N(0,1)")
knitr::kable(chi.miss, caption = "chisq (5)")
```

From the results, wee can see that the confidence intervals for standard normal population from Bootstrap method are better than $\chi^2(5)$ distributions.

Also, for standard normal population, its percentile confidence interval is better than the other two confidence intervals. For $\chi^2(5)$ distributions, the three kinds of confidence interval are similar. The the proportion of times that the confidence intervals miss on the left is fewer than the proportion of times that the confidence intervals miss on the right for the three kinds of confidence intervals. Its normal confidence interval is similar to basic confidence interval, but its percentile confidence interval has fewer proportion on the left and more proportion on the rigth.

## Question 7.8

Efron and Tibshirani discuss the following example. The five dimensional scores data have a $5\times 5$ covariance matrix $\Sigma$, with positive eigenvalues $\lambda_1>\lambda_2>...<\lambda_5$. In principal componenets analysis, 
$$
\theta = \frac {\lambda_1}{\sum^5_{j=1}\lambda_j}
$$ 
measures the proportion of variance explained by the firt principal component. Let $\hat{\lambda}_1>...>\hat{\lambda}_5$ be the eigenvalues of $\hat{\Sigma}$, where $\hat{\Sigma}$ is the MLE of $\Sigma$. Compute the sample estimate
$$
\hat\theta = \frac {\hat\lambda_1}{\sum^5_{j=1}\hat\lambda_j}
$$
of $\theta$. Obtain the jackknife estimates of bias and standard error of $\hat\theta$.

## Answer

```{r}
library(bootstrap)
r<-function(x){
# compute theta given in the question from a sample
  cov<-cov(x)
  eigenvalues<-eigen(cov)$values
  theta<-eigenvalues[1]/sum(eigenvalues)
  return(theta)
}
theta.hat<-r(scor)
n<-nrow(scor)
theta.jack<-numeric(n)
for (i in 1:n) {
  theta.jack[i]<-r(scor[-i,])
}
bias<-(n-1)*(mean(theta.jack)-theta.hat)
se<-sqrt((n-1)*mean((theta.jack-mean(theta.jack))^2))
print(c(theta.hat,bias,se))
```

## Question 7.10

In example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the corss validation procedure? Which model is selected according to maximum adjuested $R^2$?

## Answer

```{r}
library(DAAG)
attach(ironslag)
m<-length(magnetic)
e1<-e2<-e3<-e4<-numeric(m)
Rsquare<-numeric(4)
for (k in 1:m) {
  y<-magnetic[-k]
  x<-chemical[-k]
  
  J1<-lm(y~x)
  yhat1<-J1$coef[1]+J1$coef[2]*chemical[k]
  e1[k]<-magnetic[k]-yhat1
  
  J2<-lm(y~x+I(x^2))
  yhat2<-J2$coef[1]+J2$coef[2]*chemical[k]+J2$coef[3]*chemical[k]^2
  e2[k]<-magnetic[k]-yhat2
  
  J3<-lm(log(y)~x)
  logyhat3<-J3$coef[1]+J3$coef[2]*chemical[k]
  yhat3<-exp(logyhat3)
  e3[k]<-magnetic[k]-yhat3
  
  J4<-lm(y~x+I(x^2)+I(x^3))
  yhat4<-J4$coef[1]+J4$coef[2]*chemical[k]++J4$coef[3]*chemical[k]^2++J4$coef[4]*chemical[k]^3
  e4[k]<-magnetic[k]-yhat4
}
c(mean(e1^2),mean(e2^2),mean(e3^2),mean(e4^2))

L1<-lm(magnetic~chemical)
Rsquare[1]<-summary(L1)$r.squared

L2<-lm(magnetic~chemical+I(chemical^2))
Rsquare[2]<-summary(L2)$r.squared

L3<-lm(log(magnetic)~chemical)
Rsquare[3]<-summary(L3)$r.squared

L4<-lm(magnetic~chemical++I(chemical^2)++I(chemical^3))
Rsquare[4]<-summary(L4)$r.squared

Rsquare
```

From the results, a quadratic model is selected by the corss validation procedure. But a cubic polynomial model is selected according to maximum adjuested $R^2$

## Question 

8.3
The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implements a permution test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

## Answer

```{r}
set.seed(0)
maxout<-function(x,y){
  X<-x-mean(x)
  Y<-y-mean(y)
  outx<-sum(X>max(Y))+sum(X<min(Y))
  outy<-sum(Y>max(X))+sum(Y<min(X))
  return(max(c(outx,outy)))
}
n1<-15
n2<-20
mu1<-mu2<-0
sigma1<-1
sigma2<-1
m<-1000
stat<-replicate(m,expr = {
  x<-rnorm(n1,mu1,sigma1)
  y<-rnorm(n2,mu2,sigma2)
  maxout(x,y)
})
print(cumsum(table(stat))/m)
print(quantile(stat,.95))
```

```{r}
library(boot)
set.seed(3)
count5test<-function(z,ix,size){
  n1<-size[1]
  n2<-size[2]
  n<-n1+n2
  if(is.vector(z)) z<-data.frame(z,0); z<-z[ix,];
  X<-z[1:n1,1]
  Y<-z[(n1+1):n,1]
  X<-X-mean(X)
  Y<-Y-mean(Y)
  outx<-sum(X>max(Y))+sum(X<min(Y))
  outy<-sum(Y>max(X))+sum(Y<min(X))
  return(max(c(outx,outy)))
}
mu1<-mu2<-0
# if not, we can let x<-x-mean(x)
sigma1<-2
sigma2<-2.5
n1<-15
n2<-25
x<-rnorm(n1,mu1,sigma1)
y<-rnorm(n2,mu2,sigma2)
z<-c(x,y)
N<-c(n1,n2)
boot.obj<-boot(z,statistic = count5test,sim="permutation",R=999,size=N)
tb<-c(boot.obj$t0,boot.obj$t)
mean(tb>=tb[1])
```

## Question

Powercomparison:

Model 1： $Y=X/4+e$

Model 2:  $Y=X/4\times e$

$X\sim N(0_2,I_2),e\sim N(0_2,I_2)$, $X$ and $e$ are independent.

## Answer

```{r}
library(boot)
library(MASS)
library(Ball)
set.seed(0)
dCov<-function(x,y){
  x<-as.matrix(x)
  y<-as.matrix(y)
  n<-nrow(x)
  m<-nrow(y)
  if (n!=m || n<2) stop("Sample sizes must agree")
  if (!(all(is.finite(c(x,y)))))
    stop("Data contains missing or infinite values")
  Akl<-function(x){
    d<-as.matrix(dist(x))
    m<-rowMeans(d)
    M<-mean(d)
    a<-sweep(d,1,m)
    b<-sweep(a,2,m)
    b + M
  }
  A<-Akl(x);B<-Akl(y)
  sqrt(mean(A*B))
}
ndCov2<-function(z,ix,dims){
  p<-dims[1]
  q<-dims[2]
  d<-p+q
  x<-z[,1:p]
  y<-z[ix,-(1:p)]
  return(nrow(z)*dCov(x,y)^2)
}
mu<-c(0,0)
sigma<-diag(1,2)
m<-100
pow<-function(n,type){
  p.values<-matrix(0,m,2)
  for (i in 1:m) {
    X<-mvrnorm(n,mu,sigma)
    e<-mvrnorm(n,mu,sigma)
    if(type==1) Y<-X/4+e
    if(type==2) Y<-X/4*e
    z<-cbind(X,Y)
    boot.obj<-boot(data = z, statistic = ndCov2,R=99,sim = "permutation",dims=c(2,2))
    tb<-c(boot.obj$t0,boot.obj$t)
    p.values[i,1]<-mean(tb>=tb[1])
    p.values[i,2]<-bcov.test(X,Y,R=99,seed = i)$p.value
  }
  alpha<-0.05
  pow0<-colMeans(p.values<alpha)
  return(pow0)
}
N<-c(5,10,20,30,50,90)
power1<-power2<-matrix(0,6,2)
for (i in 1:6) {
  power1[i,]<-pow(N[i],1)
  power2[i,]<-pow(N[i],2)
}
plot(N,power1[,1],type = "l",col = 1,lty=1,ylab = "power",ylim = c(0,1),main = "Y=X/4+e")
lines(N,power1[,2],col = 2,lty=2)
legend("bottomright",legend=c("Distance correlation","Ball covariance"),col=c(1,2),lty=c(1,2))  
plot(N,power2[,1],type = "l",col = 1,lty=1,ylab = "power",ylim = c(0,1),main = "Y=X/4*e")
lines(N,power2[,2],col = 2,lty=2)
legend("bottomright",legend=c("Distance correlation","Ball covariance"),col=c(1,2),lty=c(1,2))  
```

## Question

9.4 Implement a random walk Metropolis sampler for generating the standard Laplace distribution. For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

## Answer

```{r}
library(GeneralizedHyperbolic)
set.seed(1)
f<-function(x) 0.5*exp(-abs(x))
rw.Metropolos<-function(sigma,x0,N){
  x<-numeric(N)
  x[1]<-x0
  u<-runif(N)
  k<-0
  for (i in 2:N) {
    y<-rnorm(1,x[i-1],sigma)
    if (u[i]<=f(y)/f(x[i-1]))
      x[i]<-y 
    else {
      x[i]<-x[i-1]
      k<-k+1
    }
  }
  return(list(x=x,k=k))
}
N<-2000
sigma<-c(.05,.5,2,16)
x0<-25
rw1<-rw.Metropolos(sigma[1],x0,N)
rw2<-rw.Metropolos(sigma[2],x0,N)
rw3<-rw.Metropolos(sigma[3],x0,N)
rw4<-rw.Metropolos(sigma[4],x0,N)
refline <- qskewlap(c(.025, .975))
##par(mfrow = c(2, 2))
plot(1:2000,rw1$x,type = "l",xlab = paste("sigma=",sigma[2]),ylab = "x")
abline(h=refline)
plot(1:2000,rw2$x,type = "l",xlab = paste("sigma=",sigma[2]),ylab = "x")
abline(h=refline)
plot(1:2000,rw3$x,type = "l",xlab = paste("sigma=",sigma[3]),ylab = "x")
abline(h=refline)
plot(1:2000,rw4$x,type = "l",xlab = paste("sigma=",sigma[4]),ylab = "x")
abline(h=refline)
#acceptance rate
print(c(1-rw1$k/N,1-rw2$k/N,1-rw3$k/N,1-rw4$k/N))
```

## Question

A-B-O blood type problem:

Let the three alleles be A, B, and O with allele frequencies $p,q,r$.
The 6 genotype frequencies under HWE and complete counts as follows.

|Genotype |AA| BB |OO| AO| BO| AB| Sum |
|:------:|:------:|:------:|:------:|:------:|:------:|:------:|:------:|
|Frequency | $p^2$ |$q^2$| $r^2$| 2pr| 2qr| 2pq |1 |
|Count| $n_{AA}$ |$n_{BB}$ |$n_{OO}$ |$n_{AO}$| $n_{BO}$| $n_{AB}$| n|

Observed data: $n_{A\cdot}=n_{AA}+n_{AO}=28$ (A-type), $n_{B\cdot}=n_{BB}+n_{BO}=24$ (B-type), $n_{OO}=41$ (O-type), $n_{AB}=70$ (AB-type).
    
Use EM algorithm to solve MLE of $p$ and $q$ (consider missing data $n_{AA}$ and $n_{BB}$).
    
Record the log-maximum likelihood values in M-steps, are they increasing?

```{r}
LL<-function (theta,theta0){
  p<-theta[1]
  p0<-theta0[1]
  q<-theta[2]
  q0<-theta0[2]
  r<- 1-p-q
  r0<- 1-p0-q0
  l<- 2*41*log(r)+70*log(2*p*q)+2*28*log(p)+2*24*log(q)+2*r0/(p0+2*r0)*log(2*r/p)+2*r0/(q0+2*r0)*log(2*r/q)
  return(-l)
}
theta0<-c(.2,.5)
n<-100
h<-numeric(n)
try(
  for (i in 1:n) {
  theta<-optim(theta0,LL,theta0=theta0)$par
  h[i]<-LL(theta,theta0)
  theta0<-theta
})
cat ("p=",theta[1],"\n")
cat ("q=",theta[2],"\n")
h[1:10]
```

In question, the results converge in high speed.
However, we still can see that the log-maximum likeihood values are increasing. 

## Question 11.1

The natrual logarithm and exponential functions are inverses of each other, so that mathematically $\log(\exp x)=\exp(\log x)=x$. Show by example that this property does not hold exactly in computer arithmetic. Does the identity hold with near equality? (See all.epual.)

## Answer

```{r}
set.seed(0)
n<-1000
x<-rchisq(n,2)
mean(log(exp(x))==exp(log(x))&log(exp(x))==x)
k<-numeric(n)
for (i in 1:n) {
  k[i]=isTRUE(all.equal(log(exp(x[i])),exp(log(x[i])))) & isTRUE(all.equal(log(exp(x[i])),x[i]))
}
mean(k)
```


I choose 1000 samples from $\chi^2(2)$.
Then,  I compute the probability of $\log(\exp x)  = \exp(\log x) = x$.
From the result, we can know that the mathematical property $\log(\exp x)=\exp(\log x)=x$ may not hold in computer arithemtic.
Meanwhile, the identity hold with near equality.

## Question 11.5

Write a function to solve the equation
$$
\frac{2\Gamma \left( \frac k 2 \right)}{\sqrt{\pi(k-1)}\Gamma \left(\frac{k-1}{2} \right)}
\int_0^{c_{k-1}} \left( 1+\frac{u^2}{k-1} \right)^{-k/2}du=\frac{2\Gamma \left( \frac {k+1} 2 \right)}{\sqrt{\pi k}\Gamma \left(\frac{k}{2} \right)}\int_0^{c_k} \left( 1+\frac{u^2}{k} \right)^{-(k+1)/2}du
$$
for a, where 
$$
c_k=\sqrt{\frac{a^2k}{k+1-a^2}}.
$$
Compare the solutions with the points $A(k)$ in Exercise 11.4.

11.4

Find the intersection points $A(k)$ in $(0,\sqrt k)$ of the curves
$$
S_{k-1}(a)=P\left( t(k-1)>\sqrt{\frac{a^2(k-1)}{k-a^2}}\right)
$$
and
$$ 
S_k(a)=P\left( t(k)>\sqrt{\frac{a^2k}{k+1-a^2} } \right),
$$
for $k=4:25,100,500,1000$, where $t(k)$ is a Student $t$ random variable with $k$ degrees of freedom.


## Answer

We first compute the $A(k)$ in 11.4

```{r}
k<-c(4:25,100,500,1000)
root<-numeric(length(k))
f<-function(a,k){
  pt(sqrt(a^2*(k-1)/(k-a^2)),df=k-1)-pt(sqrt(a^2*(k)/(k+1-a^2)),df=k)
}
for (i in 1:length(k)) 
  root[i]<-uniroot(f,interval = c(0.1,sqrt(k[i]-0.1)),k=k[i])$root
root
```

From this, we can see that when $k$ is large (in this sample data, $k\ge 23$), the roots from the function may be away from the real ones.

Then, we solve question 11.5.
When $k\ge 23$,the results in 11.4 are not closed to the reals ones, what is more, we can not use the function uniroot.
Thus, we just solve the problem with $k\le 22$.


```{r}
c<-function(k,a){
  sqrt(a^2*k/(k+1-a^2))
}
g<-function(x,m){
  2*gamma((m+1)/2)/(sqrt(pi*m)*gamma(m/2))*(1+x^2/m)^(-(m+1)/2)
}
h<-function(a,k){
  integrate(g,lower = 0,upper = sqrt(a^2*k/(k+1-a^2)),m=k)$value-integrate(g,lower = 0,upper = sqrt(a^2*(k-1)/(k-a^2)),m=k-1)$value
}
root1<-numeric(19)
for (j in 1:19) {
  l<-k[j]
  root1[j]<-uniroot(h,lower = 0.1,upper = sqrt(l)-0.1,k=l)$root
}
root1
##comparison
delta<-a0<-numeric(19)
for (i in 1:19) {
  delta[i]<-root[i]-root1[i]
  a0[i]<-isTRUE(all.equal(root[i],root1[i]))
}
## the differences between root and root1
delta
## whether the root and equals root1 with near equality  
mean(a0)
```
From the results, we can see that the ponits $A(k)$ are a little different from those in Exercise 11.4


```{r}
LL<-function (theta,theta0){
  p<-theta[1]
  p0<-theta0[1]
  q<-theta[2]
  q0<-theta0[2]
  r<- 1-p-q
  r0<- 1-p0-q0
  l<- 2*41*log(r)+70*log(2*p*q)+2*28*log(p)+2*24*log(q)+2*r0/(p0+2*r0)*log(2*r/p)+2*r0/(q0+2*r0)*log(2*r/q)
  return(-l)
}
theta0<-c(.2,.5)
n<-100
h<-numeric(n)
try(
  for (i in 1:n) {
  theta<-optim(theta0,LL,theta0=theta0)$par
  h[i]<-LL(theta,theta0)
  theta0<-theta
})
cat ("p=",theta[1],"\n")
cat ("q=",theta[2],"\n")
h[1:10]
```


## Questions  P204

3.Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:

formulas <-list(

mpg ~ disp,

mpg ~I(1/ disp),

mpg ~ disp + wt,

mpg ~I(1/ disp) + wt

)

```{r}
formulas <-list(
mpg ~ disp,
mpg ~I(1/ disp),
mpg ~ disp + wt,
mpg ~I(1/ disp) + wt
)
#lapply
lapply(seq_along(formulas),function(i){lm(formula = formulas[[i]],data = mtcars)})
#for loop
out1<-list()
for (i in seq_along(formulas)) {
  out1[[i]]<-lm(formula = formulas[[i]],data = mtcars)
}
out1
```


4.Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function?

bootstraps <-lapply(1:10, function(i) {

rows <-sample(1:nrow(mtcars),rep =TRUE)

mtcars[rows, ]

})

```{r}
set.seed(123)
bootstraps <-lapply(1:10, function(i) {
rows <-sample(1:nrow(mtcars),rep =TRUE)
mtcars[rows, ]
})
#fit<-function(dat){lm(formula = mpg ~ disp,data = dat)}
#lapply
lapply(bootstraps, lm,formula = mpg ~ disp)
#for loop
out2<-vector("list", length(bootstraps))
for (i in seq_along(bootstraps)) {
  out2[[i]]<-lm(formula = mpg ~ disp,data = bootstraps[[i]])
}
out2
```


5.For each model in the previous two exercises, extract $R^2$ using the function below.

rsq <- function(mod) summary(mod)$r.squared

```{r}
rsq <- function(mod)summary(mod)$r.squared
lapply(out1, rsq)
lapply(out2, rsq)
```

## Question   P214

3.The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.

trials <-replicate(

100,

t.test(rpois(10,10),rpois(7,10)),

simplify = FALSE

)

Extra challenge: get rid of the anonymous function by using [[directly.

```{r}
set.seed(123)
trials <-replicate(
100,
t.test(rpois(10,10),rpois(7,10)),
simplify = FALSE
)
sapply(trials, function(obj){obj$p.value})
sapply(trials,"[[",3)
```

7.Implement mcsapply(), a multicore version of sapply(). Can you implement mcvapply(), a parallel version of vapply()? Why or why not?

Because I can not use mclapply in windows when the value of mc.cores is larger than 1, I find a function "parSapply()" in  "parallel" package.

We can use it directly as a multicore version of sapply().

However, I do not think a parallel version of vapply() exists.
It is beacause vapply() takes an additional argument specifying the output type.
If ones takes the argument in every step, the result must be wrong.
Thus, I do not think a parallel version of vapply() exists.

```{r}
library(parallel)
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
func<-function(x){return(x+1)}
system.time(sapply(1: 5000000, func))
system.time(parSapply(cl,1: 5000000, func))
stopCluster(cl)
```


